<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_l5ougy15q15g-0.start{counter-reset:lst-ctn-kix_l5ougy15q15g-0 0}.lst-kix_l5ougy15q15g-3>li{counter-increment:lst-ctn-kix_l5ougy15q15g-3}ol.lst-kix_l5ougy15q15g-4.start{counter-reset:lst-ctn-kix_l5ougy15q15g-4 0}.lst-kix_l5ougy15q15g-4>li{counter-increment:lst-ctn-kix_l5ougy15q15g-4}ol.lst-kix_l5ougy15q15g-7.start{counter-reset:lst-ctn-kix_l5ougy15q15g-7 0}ol.lst-kix_l5ougy15q15g-1.start{counter-reset:lst-ctn-kix_l5ougy15q15g-1 0}ol.lst-kix_l5ougy15q15g-4{list-style-type:none}ol.lst-kix_l5ougy15q15g-5{list-style-type:none}ol.lst-kix_l5ougy15q15g-6{list-style-type:none}ol.lst-kix_l5ougy15q15g-7{list-style-type:none}ol.lst-kix_l5ougy15q15g-0{list-style-type:none}ol.lst-kix_l5ougy15q15g-1{list-style-type:none}ol.lst-kix_l5ougy15q15g-2{list-style-type:none}ol.lst-kix_l5ougy15q15g-3{list-style-type:none}ol.lst-kix_l5ougy15q15g-8.start{counter-reset:lst-ctn-kix_l5ougy15q15g-8 0}ol.lst-kix_l5ougy15q15g-8{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-1{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-0{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-5{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-4{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-3{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-2{list-style-type:none}.lst-kix_l5ougy15q15g-1>li{counter-increment:lst-ctn-kix_l5ougy15q15g-1}ol.lst-kix_l5ougy15q15g-5.start{counter-reset:lst-ctn-kix_l5ougy15q15g-5 0}.lst-kix_l5ougy15q15g-6>li{counter-increment:lst-ctn-kix_l5ougy15q15g-6}.lst-kix_l5ougy15q15g-7>li{counter-increment:lst-ctn-kix_l5ougy15q15g-7}.lst-kix_l5ougy15q15g-0>li{counter-increment:lst-ctn-kix_l5ougy15q15g-0}ol.lst-kix_l5ougy15q15g-2.start{counter-reset:lst-ctn-kix_l5ougy15q15g-2 0}ul.lst-kix_5ulxpqfdeo8-8{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-7{list-style-type:none}ul.lst-kix_5ulxpqfdeo8-6{list-style-type:none}.lst-kix_l5ougy15q15g-2>li{counter-increment:lst-ctn-kix_l5ougy15q15g-2}ol.lst-kix_l5ougy15q15g-6.start{counter-reset:lst-ctn-kix_l5ougy15q15g-6 0}.lst-kix_5ulxpqfdeo8-6>li:before{content:"-  "}.lst-kix_l5ougy15q15g-2>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-2,lower-roman) ". "}.lst-kix_l5ougy15q15g-3>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-3,decimal) ". "}.lst-kix_5ulxpqfdeo8-5>li:before{content:"-  "}.lst-kix_5ulxpqfdeo8-7>li:before{content:"-  "}.lst-kix_l5ougy15q15g-5>li{counter-increment:lst-ctn-kix_l5ougy15q15g-5}.lst-kix_l5ougy15q15g-4>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-4,lower-latin) ". "}.lst-kix_l5ougy15q15g-0>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-0,decimal) ". "}.lst-kix_l5ougy15q15g-8>li{counter-increment:lst-ctn-kix_l5ougy15q15g-8}.lst-kix_5ulxpqfdeo8-2>li:before{content:"-  "}.lst-kix_5ulxpqfdeo8-1>li:before{content:"-  "}.lst-kix_l5ougy15q15g-1>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-1,lower-latin) ". "}.lst-kix_5ulxpqfdeo8-0>li:before{content:"-  "}.lst-kix_5ulxpqfdeo8-8>li:before{content:"-  "}ol.lst-kix_l5ougy15q15g-3.start{counter-reset:lst-ctn-kix_l5ougy15q15g-3 0}.lst-kix_l5ougy15q15g-8>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-8,lower-roman) ". "}.lst-kix_l5ougy15q15g-6>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-6,decimal) ". "}.lst-kix_l5ougy15q15g-7>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-7,lower-latin) ". "}.lst-kix_5ulxpqfdeo8-3>li:before{content:"-  "}.lst-kix_5ulxpqfdeo8-4>li:before{content:"-  "}.lst-kix_l5ougy15q15g-5>li:before{content:"" counter(lst-ctn-kix_l5ougy15q15g-5,lower-roman) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c3{-webkit-text-decoration-skip:none;font-weight:700;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c2{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c23{font-weight:700;font-size:30pt;font-family:"Times New Roman"}.c11{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c17{font-weight:700;font-size:18pt;font-family:"Times New Roman"}.c16{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c8{text-decoration:none;font-size:12pt;font-family:"Times New Roman"}.c1{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c15{color:inherit;text-decoration:inherit}.c21{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c13{margin-left:36pt;padding-left:0pt}.c4{font-weight:700;font-style:italic}.c6{color:#000000;vertical-align:baseline}.c12{text-decoration:none;font-style:italic}.c20{padding:0;margin:0}.c19{color:#24292e}.c22{background-color:#00ff00}.c10{background-color:#ffffff}.c7{text-indent:36pt}.c9{height:12pt}.c14{font-style:italic}.c18{font-size:12pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Times New Roman";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:12pt;font-family:"Times New Roman"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.5;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c10 c21"><p class="c5"><span class="c2 c23">Jigsaw Puzzle Solver</span></p><p class="c5"><span class="c2 c17">Midterm Project Update</span></p><p class="c5"><span class="c2 c11">The Computer Visionaries:</span></p><p class="c5 c7"><span class="c2 c11">Beau Mitchell</span></p><p class="c5 c7"><span class="c2 c11">Guillaume Noziere</span></p><p class="c5 c7"><span class="c2 c11">Jack Humphries</span></p><p class="c5 c7"><span class="c2 c11">Kevin Lun</span></p><p class="c5 c7"><span class="c11">Matt Sternberg<br>CS 4476 : Intro to Computer Vision<br>Georgia Tech Fall 2018<br>Final Project</span></p><hr><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c2 c11">Abstract:</span></p><p class="c5 c7"><span class="c1">We wish to take an image of a medium sized (100 piece) puzzle, and output the completed puzzle image. A program to reconstruct larger puzzles has not been developed yet, so far as we can tell. Our approach has relied heavily on the work done by Travis Allen [2], who was able to solve very simple real-life jigsaw puzzles, and Nemanja </span><span class="c1 c10 c19">Mili&#263;evi&#263;&rsquo;s</span><span class="c1">&nbsp;[5]. Travis Allen&rsquo;s work shows how to process and solve a small puzzle using an picture. Nemanja </span><span class="c1 c19 c10">Mili&#263;evi&#263;&rsquo;s </span><span class="c1">GAPS algorithm can be used to quickly solve the larger puzzles Allen couldn&rsquo;t. The work done in phase 1 tested various types of algorithms and fitness functions on different rectangular puzzle sizes. This was done so in phase 2 we can focus on digitizing and solving physical jigsaw puzzles with our algorithms. The digitization of pieces followed the work done by Travis Allen, but was augmented further to gain better performance. Initially, a the piece bitmask was extracted using a threshold value and then bitmask edges were graphed in polar coordinates to find the corners at local maxima. We got much worse results from this than </span><span class="c1 c10">Travis Allen did though so augmented our code with an approach proposed by </span><span class="c1">Albertazzi [7]. The end results of our experiments are underwhelming. Our future work and conclusion sections explore future experiments that could improve our algorithm.</span></p><hr><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c2 c11">Teaser Figures:</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 234.50px; height: 313.99px;"><img alt="" src="images/image17.png" style="width: 234.50px; height: 313.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 275.74px; height: 330.50px;"><img alt="" src="images/image2.png" style="width: 823.27px; height: 377.71px; margin-left: -278.38px; margin-top: -24.32px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 240.14px; height: 293.50px;"><img alt="" src="images/image4.png" style="width: 442.02px; height: 332.59px; margin-left: -97.75px; margin-top: -22.03px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 241.54px; height: 292.50px;"><img alt="" src="images/image10.png" style="width: 445.92px; height: 333.88px; margin-left: -96.47px; margin-top: -21.40px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c9"><span class="c2 c11"></span></p><p class="c0"><span class="c2 c11">Top Left: Input Image</span></p><p class="c0"><span class="c2 c11">Top Right: Thresholded Piece</span></p><p class="c0"><span class="c2 c11">Bottom Left: Output Bitmask</span></p><p class="c0"><span class="c2 c11">Bottom Left: Found Corners</span></p><hr><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c2 c11">Introduction:</span></p><p class="c5"><span class="c11 c14">Motivation:</span></p><p class="c5"><span class="c2 c1">We chose this project mostly out of intellectual curiosity, but there are some practical applications that can be derived out of optimal puzzle reconstruction. For instance, optimal piece reconstruction can possibly be used towards automatic piecing-together of archeological finds, stitching back together a shredded document or picture, or finding the optimal part alignments in some engineering applications. It would be cool to give a program a picture of a jigsaw puzzle and have it output the solved puzzle (opens the door for the algorithm to tell you how to solve it).</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c11 c6 c12">Applications:</span></p><p class="c5"><span class="c2 c1">Potentially useful for reconstruction of items given an image (plane crash, broken glass).</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c11 c6 c12">New Approach:</span></p><ol class="c20 lst-kix_l5ougy15q15g-0 start" start="1"><li class="c5 c13"><span class="c2 c1">Take in input image</span></li><li class="c5 c13"><span class="c2 c1">Segment image into pieces</span></li><li class="c5 c13"><span class="c2 c1">Characterize pieces by shape and edge values</span></li><li class="c5 c13"><span class="c2 c1">Construct border of the puzzle using solver</span></li><li class="c5 c13"><span class="c2 c1">Fill in interior of the puzzle using solver</span></li><li class="c5 c13"><span class="c2 c1">Process solved puzzle so the output image is a continuous image</span></li></ol><p class="c5"><span class="c2 c1">Steps 1 through 5 were previously achieved by Allen on small puzzles. We wish to achieve better performance by using more powerful algorithms to solve the puzzles and by better processing the input pieces. For this project update, we focused solely on testing out different algorithms for solving puzzles, given simple puzzle representations (square, regularly-shaped pieces, rather than real-life jigsaw pieces).</span></p><p class="c5 c9"><span class="c2 c11"></span></p><hr><p class="c5 c9"><span class="c2 c11"></span></p><p class="c5"><span class="c11">Approach:</span></p><p class="c5"><span class="c11 c6 c12">Piece Detection / Edge Preprocessing:</span></p><p class="c5"><span class="c2 c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In order to feed the puzzle-pieces into our learning models, we needed to come up with a representation for those pieces. We achieved this by first taking pictures of the individual pieces oriented correctly (Note: originally, we intended to find pieces regardless of orientation, but the time constraints of the project caused us to adopt an assumption of correct orientation). These pictures were then fed into a piece detection algorithm that attempted to extract usable features from bitmasks of the input images (seen below), such as the edge pixels for each side of the jigsaw piece, and whether each side respectively contained a jigsaw inset, head, or neither (i.e. flat).</span></p><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, it should be mentioned that finding the bitmasks for the puzzle pieces was a crucial step in our preprocessing, and involved much fine tuning of parameters. Our main methodology of background subtraction for finding the bitmasks was to perform an inverse threshold of the image, since the pieces were all set against white backgrounds. Then, in order to remove noise from the bitmask image, we utilized morphological opening techniques (erosion, followed by dilation), with an elliptical kernel mask. An example of a bitmask for its corresponding piece can be seen </span><span>below</span><span class="c2 c1">.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 159.00px; height: 211.00px;"><img alt="" src="images/image1.jpg" style="width: 159.00px; height: 211.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 158.50px; height: 210.56px;"><img alt="" src="images/image27.jpg" style="width: 158.50px; height: 210.56px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7"><span class="c2 c1">Next, we needed to apply our edge detection algorithms on the piece bitmasks. Initially, the approach used by Travis Allen was mimicked for this piece detection algorithm. After generating the bitmasks, we were able to take the first moment of the bitmasks to find the centroid of the jigsaw piece, while also finding the edge coordinates of the jigsaw piece. From there, to find shape characteristic of each side, we converted the edge coordinates into polar form about the centroid, and tried to find local maxima in terms of edge pixel distances from the centroid. However, we noted that utilizing polar distance metrics got us stuck in local optima - in other words, it was hard to distinguish between jigsaw heads and piece corners, since they achieved similar metrics.</span></p><p class="c5 c7"><span class="c2 c1">Therefore, we decided to adopt a different approach, inspired by an article written by Riccardo Albertazzi [7]. In this approach, we first decided to find the jigsaw piece corners in order to figure out where each side of the piece would lie in the image. After computing Harris Corner values for each pixel using OpenCV&rsquo;s cornerHarris method, we thresholded the bitmask on these values and chose a certain amount of the best Harris Corners. Then, we chose our corners based on a maximizing a function for four corner points &nbsp;in the upper-right, upper-left, bottom-right, and bottom-left portions of the image. This function&rsquo;s outputs came from creating a polygon with the four points, and returned increased values for polygons that were more rectangular and had higher perimeter values.</span></p><p class="c5 c7"><span class="c1">Since we now had generated the four corner points of the jigsaw piece, we were finally able to determine where the sides of the piece were using polar coordinates, again with respect to the centroid of the bitmask, now utilizing the corners as endpoints. We utilized this fact to get the values of the edge pixels, but we were also able to use these discovered edge coordinates to find the shape of each side. By taking the median positional value of the edge coordinates on each side, we could see if there were any outlying edge points, which allowed us to estimate if a side contained a head or an inset, or if it was flat. This was crucial to creating constraints in our genetic algorithm&rsquo;s fitness function, since two puzzles with insets on neighboring sides, for instance, cannot be placed next to each other. Example outputs for our edge/side detector can be found below.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 149.50px; height: 197.88px;"><img alt="" src="images/image20.png" style="width: 339.66px; height: 254.42px; margin-left: -99.80px; margin-top: -29.90px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 148.00px; height: 198.00px;"><img alt="" src="images/image19.jpg" style="width: 148.00px; height: 198.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 150.00px; height: 198.48px;"><img alt="" src="images/image23.png" style="width: 341.61px; height: 254.48px; margin-left: -100.73px; margin-top: -29.91px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 148.00px; height: 196.61px;"><img alt="" src="images/image21.jpg" style="width: 148.00px; height: 196.61px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c2 c11">Experiments and Results:</span></p><p class="c5"><span class="c11 c14">Piece Detection / Edge Preprocessing:</span></p><p class="c5"><span class="c3">Experiment 1: Polar Thresholding to </span><span class="c16 c4">E</span><span class="c3">xtract </span><span class="c16 c4">E</span><span class="c3">dge </span><span class="c16 c4">P</span><span class="c3">ixel </span><span class="c16 c4">V</span><span class="c3 c6">alues</span></p><p class="c5"><span class="c11 c14">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c2 c1">Initially we followed the work done by Travis Allen [2] to extract edge features. The steps for doing so were as follows. First the input image was loaded in. Then it was converted to grayscale because this resulted in better performance. The greyscale piece was fed into a bitmask extraction algorithm. To extract the piece bitmask, the greyscale piece was thresholded using cv2s threshold function. For each piece the lower threshold value started at 170 and was lowered in intervals of 2 until a suitable bitmask is found. Edge detection is performed on the greyscale piece as well. The thresholded image and edge points are combined to clean up noise in the end bitmask. Next, to filter out more noise and smooth the edges, a round of erosion and dilation is performed. We aggressively filter out as much noise as possible, at the cost of edge accuracy, because finding the corners is extremely hard if any non-piece points make it into the bitmask. Finally several tests are performed to see if the found bitmask is suitable. </span></p><p class="c5"><span class="c2 c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Once a suitable bitmask is found, we move on to trying to extract the corners. This is done by first finding the center of the bitmask piece. Then we get the edges of the piece. We then plot the edges as polar coordinates centered at the pieces center. Here is where our approach begins to differ from that of Allen&rsquo;s. Allen used a matlab function to find local maximas (ie the corners). We attempted to use numpy&rsquo;s equivalent function, but found it to perform poorly. Instead we tried to find local maximas by removing all edge points below a certain magnitude and finding the maxima of the resulting peaks. The threshold magnitude value was manipulated in several ways until a suitable set of corner points was found. </span></p><p class="c5"><span class="c2 c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finally, with our corner points found, we extracted the edge points of each piece as features by putting all edge points found between corner points into their own side arrays.</span></p><p class="c5"><span class="c2 c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A fitness function that compared edge values between two pieces was made to test how well the psf algorithm could solve our puzzle using the extracted edge points.</span></p><p class="c5 c7"><span class="c1">The below images walk through a standard result of our first experiment</span><span class="c2 c1">.</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 185.63px; height: 247.50px;"><img alt="" src="images/image17.png" style="width: 185.63px; height: 247.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 204.00px; height: 266.00px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 299.00px; margin-left: -210.00px; margin-top: -16.50px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 213.00px; height: 259.00px;"><img alt="" src="images/image9.png" style="width: 624.00px; height: 299.00px; margin-left: -208.00px; margin-top: -20.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The </span><span>top </span><span class="c1">image shows the input piece. The</span><span>&nbsp;bottom left</span><span class="c1">&nbsp;image shows the output of the cv2 threshold function when run on our input image with threshold range of 170 to 240. There is a lot of noise in the top left hand corner of the image. The </span><span>bottom right</span><span class="c2 c1">&nbsp;image shows the edges detected when a canny edge detection algorithm is run on the middle threshold image. The combination of the threshold and edges result in the below bitmask.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image12.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Faintly visible in the above bitmask is several points nowhere near the edge of the piece (see the top right specifically). This is a problem because it gunks up the process of finding the corner points correctly. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 609.50px; height: 273.00px;"><img alt="" src="images/image6.png" style="width: 738.50px; height: 299.00px; margin-left: -65.09px; margin-top: -12.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c2 c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The above image shows the edge points graphed in polar coordinates around the found center. The outlier points found at -3 are especially problematic because they are almost in line with where the actual corner is. This results in the following output when the polar edges are cut off at a certain magnitude value.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 298.67px;"><img alt="" src="images/image28.png" style="width: 624.00px; height: 298.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c2 c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This first threshold value is dominated by the outlier clusters found at -3 and -1. This results in the below points being detected as candidate corners.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 298.67px;"><img alt="" src="images/image14.png" style="width: 624.00px; height: 298.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1 c2">Eventually, the algorithm fails and restarts to find a new bitmask. This time the bitmask threshold range is 168 to 240 and results in a much cleaner bitmask.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c2 c1">This in turn results in a polar graphing that lacks the outlier clusters that can be seen below.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c2 c1">These polar edge points result in a magnitude threshold thats much cleaner and the corner points are easily found (see below).</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image18.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">Thresholding like this also finds piece heads though, which need to be filtered out. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image15.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c2 c1">This is done by finding a set of points that form a square and results in the below correct corner labelings.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">This in turn allows us to easily find the piece edges by side.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 263.50px; height: 308.11px;"><img alt="" src="images/image7.png" style="width: 476.59px; height: 358.70px; margin-left: -100.82px; margin-top: -34.49px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 279.18px; height: 318.50px;"><img alt="" src="images/image26.png" style="width: 492.11px; height: 368.96px; margin-left: -100.16px; margin-top: -32.32px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 273.50px; height: 316.75px;"><img alt="" src="images/image13.png" style="width: 493.25px; height: 370.60px; margin-left: -105.13px; margin-top: -38.01px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 269.20px; height: 311.50px;"><img alt="" src="images/image16.png" style="width: 479.94px; height: 359.96px; margin-left: -99.99px; margin-top: -33.84px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7"><span class="c2 c1">The above images show the left, bottom, right and top edges extracted from the piece. The results seen above were around average. Some pieces were more accurate, and some pieces were less so. Also a problem was that sometimes a divot resulted in edges on one side ending up as part of another side (since the divot would cut into the piece so far that our assignment algorithm though they belonged to a side they didn&rsquo;t). These issues lead us to attempt to find a better algorithm to extract piece features for our psf algorithms use.</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c3">Experiment 2: Harris Corners to </span><span class="c16 c4">F</span><span class="c3">ind </span><span class="c16 c4">P</span><span class="c3">uzzle </span><span class="c16 c4">B</span><span class="c3 c6">oundaries</span></p><p class="c5 c7"><span class="c2 c1">As noted before, while the polar thresholding technique initially seemed promising, it led to much difficulty and inaccuracy, especially when estimating the shape of each pieces&rsquo; sides. Therefore, we decided to stick with the second, slightly more complex approach, which involved finding optimal Harris corners and then extracting edges in a clockwise manner (which were the gradients farthest from the bitmask centroid), mentioned previously in our approaches. </span></p><p class="c5 c7"><span class="c2 c1">This methodology had the advantage of giving us specific boundaries for the (mostly) square puzzle pieces, which made the process of finding side shapes much easier and more reliable. In fact, as opposed to the previous experiment, this method had 100% accuracy when classifying the shape of each piece side - this method succeeded even when some of the bitmasks were a little inaccurate, since this technique was scale invariant. That is, as long as there were edges located more than about a half standard deviation (positionally) inside or outside the rectangle that was dictated by the piece corners, we were guaranteed to classify the shape correctly.</span></p><p class="c5 c7"><span class="c2 c1">The main concern, therefore, became the relative sizes of pieces. There was no way to make sure each pieces&rsquo; right sides, for instance, were the same pixel length, so the similarity functions needed to carry the burden of comparing corresponding sides with differing lengths. Furthermore, another apparent issue was that the bitmask, due to the process of dilation, was larger than the actual puzzle piece in the image, meaning that our extracted pixels were generally taken from the background, even if they were accurately shaped. To deal with this, we realized that manually subtracting a certain length from the radial coordinates of the edges would decrease the size of the extracted piece. As a result, we were mostly able to counteract that issue, as can be seen in the examples below (where the points show the found centroid and corners, and the black lines represent detected edges). </span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 221.00px; height: 260.43px;"><img alt="" src="images/image11.png" style="width: 391.96px; height: 292.59px; margin-left: -80.27px; margin-top: -21.79px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 220.50px; height: 264.60px;"><img alt="" src="images/image8.png" style="width: 390.22px; height: 294.00px; margin-left: -80.18px; margin-top: -21.38px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c11 c6 c12">Similarity Functions</span></p><p class="c5 c7"><span class="c2 c1">As we mentioned during our midterm progress report, the most important contributor to the genetic algorithm success is the fitness function used. We managed to solve the synthetic puzzle perfectly using an edge pixel difference formula. Unfortunately, since the second part of this project involved solving a physical puzzle with the only input being images of individual puzzle pieces, we were not able to reuse our similarity functions as they were written. After performing the pre-processing steps described above, we obtained two pieces of information which we believed would contribute to a great fitness function: divot/head detection and piece contour. We will now discuss the three similarity functions we derived as a result.</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c3">Approach 1: Edge Pixel Difference</span></p><p class="c5 c7"><span class="c2 c1">This approach built directly off the logic and success of the fitness function from the first part of our project. In theory, if we were able to extract the edge pixels from the image correctly and compare them to those of a potentially neighboring piece then the difference of RGB values should represent how closely matched these pieces should be. There were a few problems we had to tackle before we obtained a functional fitness function. The first was to make sure the side lengths were correctly matched. As was mentioned earlier, the images could contain pieces of very different pixels lengths. To compensate, we subsample the large side to match the length of the smaller one. This should preserve the correct spatial comparison of edges. The second was to make sure that the order in which we accessed pixel values for edges was correct so that left edge pixels at a certain height were compared to the corresponding right edge pixels. Accounting for all this, we have a function which compares the RGB pixel values of any two edges. The big assumption in this approach is that edges are extracted correctly and perfectly, something that was not the case as we will discuss later.</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c3 c6">Approach 2: Divot/Head Compatibility</span></p><p class="c5 c7"><span class="c2 c1">Since the edge pixel difference similarity function was not very useful due to the bitmasks being slightly off, we created another similarity function that only focuses on the compatibility of side types which include flat, divot and head sides. Our side type detection was more forgiving than our edge pixel extraction, so this should provide better results assuming that a puzzle only has one combination of pieces in which all pieces are compatible with each other. If the two sides we are comparing have one divot and one head, then they are shape compatible. Any other combination is not valid. After using this in the genetic algorithm, we realized that since we are not taking into account where each divot/head is located on the side, there are multiple combinations possible. In addition, our side type detection was not perfect which obviously would lead to incorrect solutions. But this should at least provide a semi-legitimate puzzle configuration while completely ignoring the actual image information inside the puzzle piece.</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c3 c6">Approach 3: Combination of Approaches 1 and 2</span></p><p class="c5"><span class="c2 c1">We then tried to combine both approaches 1 and 2 to see if we could achieve better results. This was done by giving piece combinations that have compatible edges a smaller weight. This weight is then multiplied by the edge pixel difference calculated in approach 1. The outcome was an improvement, but still not good enough. From the solution, we can see multiple pieces where the edge compatibility was almost perfect. There were also pieces that were grouped correctly based on color. However, this was not enough to overcome the errors in both edge pixel extraction and side type identification.</span></p><hr><p class="c5 c9"><span class="c2 c11"></span></p><p class="c5"><span class="c2 c11">Results:</span></p><p class="c5 c7"><span class="c2 c1">We will now briefly summarize the results of the different similarity functions we used and try to explain why they were not what we were hoping for. For each of the displayed experimental runs, we ran the genetic algorithm with population 200 and a mutation rate of 0.15. This set of parameters typically performed as well as any others, and matches those used on synthetic puzzles in our first project update. Unlike in the first project update, we extended iteration until complete convergence, that is, the algorithm performed more than 100 iterations without improving maximal fitness. We were surprised to find that tuning of these parameters (besides iteration count) resulted in minimal effects to overall performance, with higher population sizes performing about the same despite causing a significant increase in runtime.</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c4">Approach 1: S</span><span class="c4 c18">hape </span><span class="c4">O</span><span class="c4 c6 c8">nly</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 390.67px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 390.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7"><span class="c2 c1">Results for our similarity metric which only accounted for the shapes of pieces were very poor. Manual inspection of the function&#39;s performance on individual pairs of pieces indicated that there were clusters where shape compatibility was very good. &nbsp;However, this meant that compatibility between clusters was almost nonexistent.</span></p><p class="c5 c7"><span class="c2 c1">Our shape-only metric also resulted in many instances of clear local minima; configurations where individual mutations necessary to reach the optimal solution would lead to temporarily reduced fitness. For example, consider the case indicated below:</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 389.33px;"><img alt="" src="images/image25.png" style="width: 624.00px; height: 389.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7"><span class="c2 c1">Clearly, swapping the circled pieces would result in what humans would deem a better solution to the puzzle. However, the shape-based fitness metric would not find the resulting configuration significantly more fit; the same number of &quot;edge&quot; faces would be mismatched against neighbors. For this reason, despite the general robustness of the genetic optimization algorithm, the nature of the problem along with the similarity metric result in a highly non-convex search space with abundant deep and inescapable local minima. We hoped that the incorporation of color information into our fitness metric would smoothen the gradient of the search space and make optimization more effective.</span></p><hr style="page-break-before:always;display:none;"><p class="c5 c9"><span class="c11 c6 c12"></span></p><p class="c5"><span class="c4">Approach 2: Color Only</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 386.67px;"><img alt="" src="images/image24.png" style="width: 624.00px; height: 386.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7"><span class="c1">Performance was generally very poor for the similarity function which only accounted for color dif</span><span class="c2 c1">ference along the approximated edge of each piece. We saw similarities between these results and results found in the last project update, particularly when using average tile color as a similarity metric for synthetic square-tile puzzles. Imprecision introduced by real photographs of pieces made identifying corresponding pixels of adjacent pieces&#39; edges very difficult. As a consequence, when computing edge difference, our implementation computes the difference of pixels which aren&#39;t truly adjacent in the solved puzzle. We found that this issue manifested as noise, which when summed across an entire edge, smoothed out to behave more like a comparison of mean color along the edge. This, of course, is not a viable solution for solving the puzzle. As in the previous project update, this resulted in the optimizer grouping pieces with similar mean color. In the above image, dark blue parts of the ocean cluster in the bottom left, while skin color components group themselves in the upper right. This result was disheartening, but we hypothesized that injecting any amount of color information along with the shape information from Approach 1 might still yield a more tractable search space.</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c11 c6 c12">Approach 3: Combination of Approaches 1 and 2</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 389.33px;"><img alt="" src="images/image29.png" style="width: 624.00px; height: 389.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7"><span class="c2 c1">The combination of the shape based approach and color based approach did result in noticeably improved performance, but it&#39;s clear that many deep local minima in the search space still prevent the optimizer from achieving near-optimal results. The example result above demonstrates both the successes and the failures of each approach. The shape based approach correctly places most of the image&#39;s edge pieces on the appropriate edge, while the color component does a decent job of assembling clusters of consistent color. However, even this result fails to achieve nearly any correct placements of pieces. We were particularly surprised that even despite an excess of parameter tuning, this approach hardly ever yielded correct corner placements. This seems to be a side effect of the genetic algorithm&#39;s approach to mutation and crossover, combined with the exceedingly unfriendly search space of the physical jigsaw problem.</span></p><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c11 c6 c12">Approach 4: Ground Truth</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 385.33px;"><img alt="" src="images/image22.png" style="width: 624.00px; height: 385.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7"><span>Out of curiosity (or desperation), we decided to check whether or not this problem was even solvable using our current framework. The main independent variable in our setup is the fitness function so we decided to replace it with a function that returns the Manhattan distance between two pieces&rsquo; ground truth coordinates. The closer two pieces are to each other in their actual puzzle, the smaller the similarity function. When running the genetic algorithm on this setup, we obtained a solution in around 10 iterations. The result can be seen directly above. As expected, the algorithm converged on the actual solution which confirms our conception that the success of our puzzle solver relies on the accuracy of the fitness. That in turns relies on the approach used which relies on the accuracy of our preprocessing. But as we have come to realize, it is quite complicated to generate such a similarity function when starting with uncropped puzzle pieces.</span></p><hr><p class="c5 c9"><span class="c2 c11"></span></p><p class="c5"><span class="c2 c11">Conclusion:</span></p><p class="c5 c7"><span class="c1">We were unable to get our final puzzle solving algorithm to work as well as we wanted. This mainly falls on the difficulty we experienced in extracting the piece features correctly and robustly. Even though we tried several approaches, the best results we found still weren&rsquo;t good enough. For the fitness functions we derived, we very much needed each step of our puzzle-solving process to be pretty much perfect. Since the </span><span>each step of the solution had to be robust against a large space of possible jigsaw puzzle pieces, </span><span class="c1">we were not able to get strong enough individual problem solutions to obtain a strong final overall puzzle solution. </span><span>We found that small failure rates in each step of our fitness computation accumulated contributed to a high failure rate for our procedure as a whole. </span><span class="c1">We are also aware that we may have tackled a puzzle that was too large in size. Travis Allen&rsquo;s work only involved a si</span><span>ngle 3x4, 12</span><span class="c1">-piece puzzle that w</span><span>as</span><span class="c1">&nbsp;comprised mainly of brown wookie content. Our puzzle on the other was made up of 104 puzzle pieces (8-by-13). This </span><span>translated to a </span><span class="c1">configuration space </span><span>hundreds of </span><span class="c1">orders of magnitude</span><span>&nbsp;larger, with hundreds of orders of magnitude more local minima. These differences presented major challenges in computing the fitness of a configuration, and in optimizing within the massive state space</span><span class="c1">. </span><span>The image content itself presented issues even in something as simple as background subtraction. The diversity of color in the puzzle meant </span><span class="c1">it was </span><span>difficult </span><span class="c1">to find a &ldquo;perfect&rdquo; background to detect the pieces on, </span><span>particularly in comparison to Allen&#39;s original puzzle, whose consistent brown hue allowed the use of a standard green screen for background subtraction</span><span class="c1">.</span><span>&nbsp;</span><span class="c1">Still though, we are happy with the work we did in applying the concepts we learned in class to a real world problem. Failure is very common in life, especially when tackling complicated problems which so often come up in computer vision. The next section will explain why hope is still alive when it comes to finding a solution to our particular problem.</span><hr></p><p class="c5"><span class="c2 c11">Future Work:</span></p><p class="c5"><span class="c11">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c2 c1">There are many things we would we do if we had the time. Travis Allen&rsquo;s puzzle solving algorithm was much more powerful than ours because he detected without fault whether each side was a divot, head, or edge. This provides a much more accurate way to solve puzzles than by using the difference in pixel values between edges. We were unable to get this level of edge detection working 100% on our data set though. If we had more time, this would be the first step we would take to improve our algorithm. Most likely, more piece pre-processing would be involved in perfecting piece feature extraction. This pre-processing could involve machine learning techniques to make the process more robust than the brittle setup we used for our project. </span></p><p class="c5"><span class="c2 c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As well, we would try and eliminate the need for each piece to be photographed individually. Instead we would attempt to take either one big image of all the pieces, or even multiple pictures with subsets of the pieces. If our algorithm allowed multiple pictures to be taken, it would allow for higher resolution on the piece features, but would require some sort of image stitching technique such as the one we used for project 1.</span></p><p class="c5"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are several other minor fixes and changes we would make to our code base, given more time, but overall we feel content with what we accomplished.</span></p><hr><p class="c5 c9"><span class="c2 c1"></span></p><p class="c5"><span class="c2 c11">References:</span></p><p class="c5"><span class="c1">[1] </span><span class="c1"><a class="c15" href="https://www.google.com/url?q=https://arxiv.org/pdf/1711.06769.pdf&amp;sa=D&amp;ust=1543636023537000">https://arxiv.org/pdf/1711.06769.pdf</a></span></p><p class="c5"><span class="c1">[2] </span><span class="c1"><a class="c15" href="https://www.google.com/url?q=https://web.stanford.edu/class/cs231a/prev_projects_2016/computer-vision-solve__1_.pdf&amp;sa=D&amp;ust=1543636023538000">https://web.stanford.edu/class/cs231a/prev_projects_2016/computer-vision-solve__1_.pdf</a></span></p><p class="c5"><span class="c1">[3] </span><span class="c1"><a class="c15" href="https://www.google.com/url?q=http://chenlab.ece.cornell.edu/people/Andy/publications/Andy_files/Gallagher_cvpr2012_puzzleAssembly.pdf&amp;sa=D&amp;ust=1543636023539000">http://chenlab.ece.cornell.edu/people/Andy/publications/Andy_files/Gallagher_cvpr2012_puzzleAssembly.pdf</a></span></p><p class="c5"><span class="c1">[</span><span class="c2 c1">4]</span></p><p class="c5"><span class="c1"><a class="c15" href="https://www.google.com/url?q=https://medium.com/cornell-tech/jigsolved-computer-vision-to-solve-jigsaw-puzzles-70b8ad8099e5&amp;sa=D&amp;ust=1543636023539000">https://medium.com/cornell-tech/jigsolved-computer-vision-to-solve-jigsaw-puzzles-70b8ad8099e5</a></span></p><p class="c5"><span class="c2 c1">[5] https://github.com/nemanja-m/gaps</span></p><p class="c5"><span class="c1">[6] </span><span class="c1 c10">Russell, Stuart Jonathan, and Peter Norvig. </span><span class="c1 c14">Artificial Intelligence: a Modern Approach</span><span class="c2 c1 c10">. 3rd ed., Pearson, 2018.</span></p><p class="c5"><span class="c2 c1 c10">[7] https://towardsdatascience.com/solving-jigsaw-puzzles-with-python-and-opencv-d775ba730660</span></p></body></html>